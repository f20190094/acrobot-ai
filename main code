import os
os.environ['CUDA_DEVICE_ORDER']="PCI_BUS_ID"
os.environ['CUDA_VISIBLE_DEVICES']=""

# importing dependencies

import random
import gym
import numpy as np
from _collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import os

# set parameterrs

env = gym.make('Acrobot-v1')
state_size=env.observation_space.shape[0]
action_size=env.action_space.n
n_episodes=4001
output_dir='model_output/cartpole'
batch_size=32

if not os.path.exists(output_dir):
    os.makedirs(output_dir)

### modelbuilding

def DQNmodel(state_size,action_size,learning_rate):
    model = Sequential()
    model.add(Dense(24, input_dim=state_size, activation='relu'))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(action_size, activation='linear'))

    model.compile(loss='mse', optimizer=Adam(lr=learning_rate))

    return model
'''
class DQNmodel:
    def __init__(self,state_size,action_size):
        self.state_size=state_size
        self.action_size=action_size
        self.learning_rate = 0.001
    def create(self):
        model=Sequential()
        model.add(Dense(24,input_dim=self.state_size,activation='relu'))
        model.add(Dense(24,activation='relu'))
        model.add(Dense(self.action_size,activation='linear'))
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model'''

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size=state_size
        self.action_size=action_size
        self.memory=deque(maxlen=2000)
        self.gamma=0.99
        self.epsilon =1.0
        self.epsilon_decay=0.999
        self.epsilon_min=0.01
        self.learning_rate=0.001
        self.model=DQNmodel(self.state_size,self.action_size,self.learning_rate)       #
        self.sample_model=DQNmodel(self.state_size,self.action_size,self.learning_rate)

    def _build_model(self):
        model=Sequential()
        model.add(Dense(24,input_dim=self.state_size,activation='relu'))
        model.add(Dense(24,activation='relu'))
        model.add(Dense(self.action_size,activation='linear'))

        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))

        return model
    def remember(self,state,action,reward,next_state,done):
        self.memory.append((state,action,reward,next_state,done))

    def act(self,state):
        if np.random.rand()<=self.epsilon:
            return random.randrange(self.action_size)
        act_values=self.model.predict(state)        #
        return np.argmax(act_values[0])

    def replay(self,batch_size,episode):
        minibatch=random.sample(self.memory,batch_size)

        for state,action,reward,next_state,done in minibatch:
            target=reward
            if not done:
                target=(reward+self.gamma*np.max(self.sample_model.predict(next_state)[0]))        #
            target_f=self.model.predict(state)
            target_f[0][action]=target

            self.model.fit(state,target_f,epochs=1,verbose=0)
        if episode%10==0:                                                   #
            self.sample_model.set_weights(self.model.get_weights())         #
        if self.epsilon>self.epsilon_min:
            self.epsilon*=self.epsilon_decay
    def load(self,name):
        self.model.load_weights(name)

    def save(self,name):
        self.model.save_weights(name)

agent=DQNAgent(state_size,action_size)

done=False
for e in range(n_episodes):
    state=env.reset()
    state=np.reshape(state,[1,state_size])
    render=False
    #if e>980:
    #render=True
    tot_reward=0
    for time in range(500):

        if render:
           env.render()
        action=agent.act(state)
        next_state,reward,done,_=env.step(action)
        reward = reward if not done else 100
        next_state=np.reshape(next_state,[1,state_size])
        agent.remember(state,action,reward,next_state,done)

        state=next_state
        tot_reward += reward
        if done:
            print("episode: {}/{}, score: {},ee: {:.2}".format(e,n_episodes,tot_reward,agent.epsilon))
            break


    if len(agent.memory)>batch_size:
        agent.replay(batch_size,e)

    if e%50==0:
        agent.save(output_dir+"weights_"+'{:04d}'.format(e)+".hdf5")

